{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model a is [-1.11494296 -1.70482218  0.0446201   0.96751824  1.24041509]\n",
      "model c is -0.7759795581324975\n"
     ]
    }
   ],
   "source": [
    "#set up multivariate linear regression example data \n",
    "\n",
    "#define the dataset size\n",
    "num_features = 5\n",
    "num_samples = 1000\n",
    "\n",
    "#define dataset parameters \n",
    "model_a = np.random.normal(size=(num_features))\n",
    "model_c =np.random.normal()\n",
    "\n",
    "print(\"model a is {}\".format(model_a))\n",
    "print(\"model c is {}\".format(model_c))\n",
    "\n",
    "# create some random initial values and then work out the result adding in some random noise\n",
    "x_train = np.random.normal(size=(num_samples,num_features))*100\n",
    "y_train = x_train @ model_a + model_c + np.random.normal(size= num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 69665.5078125\n",
      "Epoch: 100 Loss: 29789.58984375\n",
      "Epoch: 200 Loss: 11134.5341796875\n",
      "Epoch: 300 Loss: 3785.403076171875\n",
      "Epoch: 400 Loss: 1207.9327392578125\n",
      "Epoch: 500 Loss: 359.8683166503906\n",
      "Epoch: 600 Loss: 97.5340576171875\n",
      "Epoch: 700 Loss: 23.923290252685547\n",
      "Epoch: 800 Loss: 5.783719539642334\n",
      "Epoch: 900 Loss: 1.8958250284194946\n"
     ]
    }
   ],
   "source": [
    "#use tensorflow to compute gradients and optimise the fit\n",
    "\n",
    "a = tf.Variable(tf.zeros(shape=(num_features,)),name=\"a\", trainable=True, dtype=tf.float32)\n",
    "c = tf.Variable(tf.zeros(shape=(1,)),name =\"c\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def f(x):\n",
    "    return tf.tensordot(x,a,axes=1)+c\n",
    "\n",
    "#print(tf.autograph.to_code(f.python_function))\n",
    "\n",
    "variables = [a,c]\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.005)\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "for i in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = f(tf.cast(x_train,tf.float32))\n",
    "        loss = loss_object(tf.cast(y_train,tf.float32),y_pred)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if i %100 ==0: \n",
    "            print('Epoch: {} Loss: {}'.format(i,loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit: [-1.1156785  -1.7006475   0.04418615  0.96755296  1.2408987 ] [-1.0829409]\n",
      "True :     [-1.11494296 -1.70482218  0.0446201   0.96751824  1.24041509] [-0.7759795581324975]\n"
     ]
    }
   ],
   "source": [
    "print('Model fit:',a.numpy(),c.numpy())\n",
    "print('True :    ',model_a,[model_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 1.435e+07\n",
      "Date:                Thu, 08 Oct 2020   Prob (F-statistic):               0.00\n",
      "Time:                        10:51:54   Log-Likelihood:                -1401.3\n",
      "No. Observations:                1000   AIC:                             2815.\n",
      "Df Residuals:                     994   BIC:                             2844.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.8157      0.031    -26.094      0.000      -0.877      -0.754\n",
      "x1            -1.1152      0.000  -3542.390      0.000      -1.116      -1.115\n",
      "x2            -1.7042      0.000  -5460.464      0.000      -1.705      -1.704\n",
      "x3             0.0443      0.000    138.206      0.000       0.044       0.045\n",
      "x4             0.9673      0.000   3092.000      0.000       0.967       0.968\n",
      "x5             1.2407      0.000   4135.287      0.000       1.240       1.241\n",
      "==============================================================================\n",
      "Omnibus:                        1.779   Durbin-Watson:                   1.973\n",
      "Prob(Omnibus):                  0.411   Jarque-Bera (JB):                1.690\n",
      "Skew:                          -0.049   Prob(JB):                        0.430\n",
      "Kurtosis:                       3.176   Cond. No.                         106.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [-0.81567076 -1.11516753 -1.70416648  0.04430329  0.96727484  1.24074803]\n"
     ]
    }
   ],
   "source": [
    "# statsmodels OLS .. remember to add in the constant\n",
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.OLS(y_train, sm.add_constant(x_train)).fit()\n",
    "print(model.summary(),model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit : [-1.11516753 -1.70416648  0.04430329  0.96727484  1.24074803] -0.8156707622412469\n",
      "True :      [-1.11494296 -1.70482218  0.0446201   0.96751824  1.24041509] -0.7759795581324975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_intercept': True,\n",
       " 'normalize': False,\n",
       " 'copy_X': True,\n",
       " 'n_jobs': None,\n",
       " 'coef_': array([-1.11516753, -1.70416648,  0.04430329,  0.96727484,  1.24074803]),\n",
       " '_residues': 965.249797073664,\n",
       " 'rank_': 5,\n",
       " 'singular_': array([3352.273302  , 3240.251876  , 3188.43424947, 3034.15967049,\n",
       "        3009.09722493]),\n",
       " 'intercept_': -0.8156707622412469}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SKlearn linear model\n",
    "\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x_train,y_train)\n",
    "vars(regr)\n",
    "print('Model fit :',regr.coef_,regr.intercept_)\n",
    "print('True :     ',model_a,model_c)\n",
    "vars(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit : [-1.11516754 -1.70416649  0.04430328  0.96727483  1.24074802] -0.8156707985829653\n",
      "True :      [-1.11494296 -1.70482218  0.0446201   0.96751824  1.24041509] -0.7759795581324975\n"
     ]
    }
   ],
   "source": [
    "#optimise loss function. Need to combine a+c into the first parameter \n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def cost(a,x,y):\n",
    "    predict = a[-1] + x@a[:-1]\n",
    "    return np.mean((predict-y)**2)\n",
    "\n",
    "opt_result = minimize(cost, [0,0,0,0,0,0], args=(x_train,y_train),method='BFGS', options={'maxiter': 5000})\n",
    "print('Model fit :',opt_result.x[:-1],opt_result.x[-1])\n",
    "print('True :     ',model_a,model_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
