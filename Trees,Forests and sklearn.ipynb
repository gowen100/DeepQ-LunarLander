{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "d:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
      "Data Shapes - Features (150, 4) Target: (150,) \n",
      "Iris labels are : [0 1 2]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "\n",
    "print(iris.keys())\n",
    "print(\"Data Shapes - Features {} Target: {} \".format(iris.data.shape,iris.target.shape))\n",
    "print(\"Iris labels are :\",pd.unique(iris.target))\n",
    "print(iris.feature_names)\n",
    "\n",
    "x_train, x_test ,y_train, y_test  =train_test_split(iris.data,iris.target,test_size = .33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is: \n",
      "[[ 0.63731351  1.56466997 -2.06192859 -0.96549429]\n",
      " [ 0.5024701  -0.09547246  0.02756191 -0.8466643 ]\n",
      " [-1.13978362 -1.46919751  2.03436668  1.81215859]]\n",
      "\n",
      "Accuracy is : 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(fit_intercept = False)\n",
    "lr_model.fit(x_train, y_train)\n",
    "print(\"Matrix is: \\n{}\\n\".format(lr_model.coef_))\n",
    "print(\"Accuracy is :\",sk.metrics.accuracy_score(y_test, lr_model.predict(x_test)))\n",
    "\n",
    "#print(vars(lr_model).keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] (150,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.239455\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  150\n",
      "Model:                          Logit   Df Residuals:                      148\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Fri, 28 May 2021   Pseudo R-squ.:                  0.6238\n",
      "Time:                        08:45:21   Log-Likelihood:                -35.918\n",
      "converged:                       True   LL-Null:                       -95.477\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.869e-28\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -27.8285      4.828     -5.765      0.000     -37.290     -18.367\n",
      "x1             5.1757      0.893      5.793      0.000       3.425       6.927\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "binary_y = iris.target\n",
    "binary_y[binary_y==2] = 1\n",
    "print(pd.unique(binary_y),binary_y.shape)\n",
    "\n",
    "logit = sm.Logit(binary_y,sm.add_constant(iris.data[:,:1], prepend = True))\n",
    "sm_coefs = logit.fit()\n",
    "print(sm_coefs.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- petal width (cm) <= 0.80\n",
      "|   |--- class: 0\n",
      "|--- petal width (cm) >  0.80\n",
      "|   |--- petal length (cm) <= 4.75\n",
      "|   |   |--- class: 1\n",
      "|   |--- petal length (cm) >  4.75\n",
      "|   |   |--- class: 2\n",
      "\n",
      "Accuracy is : 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "decision_tree = decision_tree.fit(x_train, y_train)\n",
    "r = export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "print(r)\n",
    "\n",
    "print(\"Accuracy is :\",sk.metrics.accuracy_score(y_test, decision_tree.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf= RandomForestClassifier(n_estimators=50,max_depth=2,max_features=.5)\n",
    "clf = clf.fit(x_train, y_train)\n",
    "#print(vars(clf).keys())\n",
    "\n",
    "print(\"Accuracy is :\",sk.metrics.accuracy_score(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbt = GradientBoostingClassifier(learning_rate =0.05,n_estimators=50,max_depth=2,min_samples_split=5,max_features=0.5)\n",
    "gbt = gbt.fit(x_train, y_train)\n",
    "print(\"Accuracy is :\",sk.metrics.accuracy_score(y_test, gbt.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from IPython.display import clear_output\n",
    "\n",
    "            \n",
    "def update_progress(progress):\n",
    "    bar_length = 30\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "    \n",
    "class progress_bar(Callback):\n",
    "        \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            update_progress((epoch+1)/500)\n",
    "\n",
    "progress = progress_bar()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [#######################-------] 75.8%\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "nn_model = Sequential([\n",
    "        Input(shape =(4,)),\n",
    "        Dense(128,activation='relu'),\n",
    "        Dense(128,activation='relu'),\n",
    "        Dense(3,activation='softmax')\n",
    "    ])\n",
    "\n",
    "nn_model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=[\"sparse_categorical_accuracy\"])\n",
    "history= nn_model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=500,callbacks=[progress], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "fig.suptitle('Neural Net Performace',fontsize=20)\n",
    "\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "ax[0].plot(history.history['sparse_categorical_accuracy'])\n",
    "ax[0].plot(history.history['val_sparse_categorical_accuracy'])\n",
    "ax[0].set_title('accuracy vs. epochs',fontsize=18)\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('Loss vs. epochs',fontsize=18)\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is :\", nn_model.evaluate(x_test,y_test,verbose= False)[1])\n",
    "for x in [\"{:3.0f} {:3.0f} {:3.0f}\".format(*i*100) for i in nn_model.predict(x_test)][:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(x_train, y_train)\n",
    "print(\"Accuracy is :\",sk.metrics.accuracy_score(y_test, gnb.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', activation = 'relu',hidden_layer_sizes=(128,128),max_iter=500)\n",
    "\n",
    "mlp = mlp.fit(x_train, y_train)\n",
    "print(\"Accuracy is :\",sk.metrics.accuracy_score(y_test, mlp.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\"{:3.0f} {:3.0f} {:3.0f}\".format(*i*100) for i in mlp.predict_proba(x_test)][:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
